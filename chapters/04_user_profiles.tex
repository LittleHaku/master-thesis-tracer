% !TeX root = ../main.tex
% Add the above to each chapter to make compiling the PDF easier in some editors.

\chapter{User Profile Structure and Generation}\label{chapter:user-profiles}

Once the model has been finished,
we use it to automatically generate user profiles for SENSEI.
These serve as test cases designed to verify
the discovered functionalities of the chatbot,
its handling of different inputs,
to check if the outputs match the expected value,
and to find other errors such as timeouts.

First, \autoref{sec:profile-structure}
will cover the structure of these profiles and how they work,
then \autoref{sec:profile-generation}
will detail how these profiles are synthesized from the inferred model.


\section{User Profiles Structure}\label{sec:profile-structure}

A user profile contains all the information
that characterises the user,
the conversation goals,
interaction style,
and other information such as the \ac{LLM} that will be used,
or the number of conversations and turn per conversations.
These profiles are structured in a YAML file,
\autoref{code:yaml-profile-pizza} and \autoref{code:yaml-profile-drinks}
show an example of the user profiles generated during a \ac{TRACER} execution
against a pizzeria chatbot made with Taskyto.

\lstinputlisting[
  language=yaml,
  caption={Example of the first user profile generated for a pizzeria chatbot.},
  label={code:yaml-profile-pizza}
]{code/pizza_orderer_and_customizer.yaml}

\lstinputlisting[
  language=yaml,
  caption={Example of the second user profile generated for a pizzeria chatbot.},
  label={code:yaml-profile-drinks}
]{code/drink_orderer.yaml}



To better understand how the profiles are generated,
we must first understand the profiles and their structure
made up of the \texttt{test\_name},
a unique identifier for the profile,
followed by four configuration blocks: 
\texttt{llm}, \texttt{user}, \texttt{chatbot} and \texttt{conversation}.

\subsection{LLM Configuration (\texttt{llm})}

In this section we can choose the \acl{LLM} that we are going to use
along with its temperature.
The chosen \texttt{model} will have an impact on the achieved results
since each model has its own strengths and limitations
and there will be models that will perform better than others,
usually at the cost of being more expensive.
In this field we will just input the name of the model (e.g. \texttt{gpt-4o-mini}).
Then we have the \texttt{temperature},
this parameter controls the randomness of the \ac{LLM},
that is, when the model is going to choose the next word
how randomly it does it.
A value of 0 means that the model is deterministic
while 1 means that it will be more creative.

\subsection{User Persona Definition (\texttt{user})}

In the \texttt{user} section, we will define
the persona, its role, context and goals.
We start with the \texttt{language} that the user will employ,
followed by the \texttt{role} of the simulated user,
that is, a sentence representing who the user is
and what he ought to do (e.g., A customer ordering a pizza).
The \texttt{context} field provides additional information to the chatbot,
it can add more details or personality traits, such as,
"You are in a hurry", to make the simulated user more realistic.
The \texttt{goals} field is arguably the most important of the user's fields.
In it, we will describe all the objectives for the conversation.
These objectives are written like templates with placeholders
(e.g., Specify the pizza size as {{pizza\_size}}).
Below the list with all the objectives
we have the list of parameters, which is the placeholders we left in goals.
Each of these parameters will have assigned a function:
\begin{itemize}
  \item \texttt{random()}: A random value of the data below will be taken.
  \item \texttt{random(x)}: Takes $x$ different random values.
  \item \texttt{forward()}: Selects one value sequentially from the data list.
  \item \texttt{forward(x)}: Instead of one by one, takes the $x$ next values.
  \item \texttt{forward(var)}: 
    Allows to cycle through all the combinations of a pair (or more, depending on the nested combinations) of variables.
    For example, one could nest pizza sizes with pizza types,
    that means that all combinations of pizza sizes with pizza types would be tested,
    that is, small pepperoni, small four chesee, ..., medium pepperoni, medium four chesee, ..., large pepperoni, large four chesee...
\end{itemize}
After the function, we can specify its \texttt{type},
with values like int, float, string or date.
Right after that, we have the data field,
where we will input a list of values
or if it is a numeric value, a range with min and max
and the step to go from the min to the max.

\subsection{Chatbot Settings (\texttt{chatbot})}

Here, expected behaviour of the system under test will be specified.
The first field is the \texttt{fallback},
this is the sentence given by the chatbot when he cannot understand what the user is saying,
or what he is saying is outside of the chatbot's scope.
Next we have the \texttt{outputs}, a field similar to the goals
but in this case instead of being related to the inputs,
it will be used to extract outputs from the chatbot.
For each output we will give a name
then a \texttt{type} which can be string, money, int, float or date;
then we have a short \texttt{description}.
This information will be used by an \ac{LLM}
to extract the information from the conversation with the chatbot.


\subsection{Conversation Control (\texttt{conversation})}

This last section controls aspects of the execution.
The \texttt{number} will control the number of times the conversation will take place,
the more conversations, the more combinations of the goals' items to be tested.
This field allows an integer, that simply indicates the number of conversations;
\texttt{all\_combinations} that will exhaustively test every combination,
although it ensures good coverage of the inputs,
it can also result in an enormous number of test cases, specially if we use nested forwards;
\texttt{sample(x)}, where $x$ is a number between $0$ and $1$,
will compute all the possible combinations
and take a percentage of all of these.

The second field, \texttt{goal\_style}, is the test's stop condition.
It can be the number of steps taken in the conversation
(let a step be a user message followed by a chatbot's one),
or \texttt{all\_answered} which will stop once the user has completed all of its goals,
this parameter is also accompanied by a \texttt{limit} field which sets a hard limit
making sure that the conversation finishes even if the chatbot is not able to fulfil the user's goals.

Lastly, we have the \texttt{interaction\_style},
which lets us set a list of styles that the simulated user will adopt for its conversations.
This styles are predefined and include some like \textit{make spelling mistakes}, \textit{use short phrases}
or \textit{single question}.

\section{User Profiles Generation}\label{sec:profile-generation}

The profile generation is an automated process
that aims to convert the inferred model
into a set of profiles that will thoroughly test the chatbot.
The process is divided into seven steps that begins by
grouping functionalities into profiles,
followed by the \ac{LLM}-driven generation of goals, variables, context, and outputs.
Then, the definition of the conversation style and,
finally, the profile assembly and validation step.
Each of these steps is detailed in the following subsections.

\subsection{Grouping Functionalities into Profiles}

The inferred model is send to an \ac{LLM} along with conversation fragments,
with this information the \ac{LLM} is prompted to
group the functionalities into realistic and logical user scenarios.
Each group will gather functionalities that are semantically related,
frequently used together or performed sequentially.
Ideally, each functionality is only assigned to one group
to ensure that all functionalities are used but avoiding redundancy;
this is not always achieved since in cases where a functionality branches into others
it is required to go through it at least twice.
From this functionality groups, the \ac{LLM} will generate
the \texttt{test\_name} along with its \texttt{role}.

\subsection{Goal Generation}

After the grouping has been done,
the group of functionalities is sent to a new \ac{LLM} call
that will create a set of functionalities that when fulfilled
will ensure that the functionalities have been activated.
The goals may or may not have variables,
it will depend on the nature of the goal,
for example if asking about the opening hours it will not have variables,
but if the goal is about asking for a pizza,
then the goal will likely have variables.

\subsection{Variable Generation}

This step is only needed when variable \texttt{\{\{placeholders\}\}} have been defined.
To understand how the variables are generated first we need to see waht a data source is.
A data source are the values of either a parameter or an output from a functionality node,
we decided to use both outputs and parameters since there are occasions
where the extracted functionality is for example "list available pizza types"
and the values that we can use for the variables
are in the outputs instead of the more obvious parameters.
So the data sources are the combinations of all the values
from all outputs and parameters.

Now that we now this, the procedure is as follows for each individual variable.
First, we pass all these data sources to the \ac{LLM}
and prompt it to match the variable to one of the data sources
emphasizing that it is not necessary to force a match
just for the sake of making one.
Then, if one match is given,
we request the \ac{LLM} to generate another extra value
based on what the returned data source
to test the chatbot on values outside of the ones that he suggests.
For example, with the pizza sizes small, medium, large,
it tends to generate extra large as the extra value,
or in the toppings pineapple was generated in this case (see \autoref{code:yaml-profile-pizza})

In the case that no data source is matched,
either because the chatbot didn't provide values
(for example with dates or numbers)
or because the data source matching didn't correctly match the variable,
the \ac{LLM} will generate the data for the variable given
the goals, functionalities, role and conversations.
It is important to note that variables are not only restricted to be strings,
in the \autoref{code:yaml-profile-drinks} the drink quantity
are a great example of variables generated
with numeric values instead of being strings or matched data sources.

The generated variables always use the \texttt{forward()} function
since it allows us to test every option.
We chose this over the nested one
since it has a great balance between completeness and number of conversations.
For example, having $8$ pizza types with $3$ different pizza sizes
with nested forwards would require $8 \cdot 3 = 24$ conversations,
while using the regular forward would require only $8$ conversations
to go through all the options.

\subsection{Context Generation}

A simple context of two or three sentences
is generated by the \ac{LLM} based on
the functionalities, the content generated before and the conversations.
The idea of the generated context is to create a more realistic scenario
where the user is not simply testing the chatbot
but in a realistic case where for example
the user is in a get together is looking to order pizzas,
or is an Erasmus student requesting help to a university chatbot.

\subsection{Output Definition}

The output definition along with the goal and variables is one of the key steps.
It will let us test the chatbot
and see if the chatbot is returning the information that it should
and thus completing its functionalities.
The outputs are generated by the \ac{LLM}
taking into account the functionality and the goals,
trying to look for things that will ensure that they have been achieved,
like for example the order ID or the order price;
and by also looking at the outputs from the functionality nodes.
The \autoref{code:yaml-profile-drinks} contains great examples
of outputs of different types such as money, int or string.

These outputs are as granular as possible
to allow for a better testing of the chatbot,
for example, instead of a more vague output like
"order confirmation", it would be split into two granular outputs
"order ID" and "total order price".

\subsection{Conversation Style Definition}

The \texttt{conversation} section does not require \ac{LLM} calls.
First, the number of conversations is chosen
based on the variable with the largest data set,
for instance, if we have four variables,
and pizza type, with $8$ options, is the one with the larger set of options,
the number of conversations will be $8$ to ensure that all the variables are tested.

For the length of the conversation,
we went for a limit that is set based
on a linear combination of the number of goals and outputs,
so that the greater the number of goals and outputs,
the longer the conversation will be,
but still with a hard limit of $30$.

Lastly, the interaction style always is set to \texttt{single\_question}
since if not the uer simulator tries to fulfil all the goals at the same time
and ends up making weird complicated sentences.

\subsection{Profile Assembly and Validation}

Once all the fields have been generated
the profile is turned into a YAML profile.
This profile is then passed through a validation script
that will check things like that all the required fields are complete,
that every placeholder has a variable defined,
and that this one is correctly defined
amongst other things,
and if any error appears it will return a verbose description of the issue.
Then, if any error arises, the description along with the YAML file
are sent to the \ac{LLM} with a prompt to fix the issue.


This seven steps allow us to turn the inferred model
from the deployed chatbot under test,
into a set of realistic and comprehensive user profiles
that will act as test cases when combined with the user simulator Sensei.
This profile generation stage bridges the gap between
black-box model inference and automated testing.
