% !TeX root = ../main.tex
% Add the above to each chapter to make compiling the PDF easier in some editors.

\chapter{Evaluation}\label{chapter:evaluation}

This section aims to
evaluate how \ac{TRACER} performs at
modelling and synthesising user profiles
from a chatbot under testing.
To achieve this, the following research questions are addressed:

\begin{itemize}
\item \textbf{RQ1: How effective is TRACER in modelling chatbot functionality?}
  This question evaluates the capability of the proposed model discovery method to attain high functional coverage in a controlled environment where the ground truth is available.
\item \textbf{RQ2: How effective are the synthesised profiles at detecting faults in controlled environments?}
  This question tests the accuracy of the proposed method by applying mutation testing \autocite{gomez-abajoMutationTestingTaskOriented2024} to estimate the capacity of the created profiles to detect specific, injected faults.
\item \textbf{RQ3: How accurately does TRACER model the functionalities of real-world, deployed chatbots?}
  This question addresses the practical, real-world applicability of \ac{TRACER}.
  To answer it, we run \ac{TRACER} against a set of deployed chatbots,
  and then perform a manual verification of every functionality inferred.
  This allows us to measure the precision of the model,
  that is, the percentage of discovered functionalities that are correct and valid.
\end{itemize}

RQ1 evaluates the coverage of the proposed approach
in terms of activating chatbot functionalities.
To measure the coverage, \ac{TRACER} will be executed
against the chatbot under testing
and the generated synthesised profiles
will be executed with SENSEI against the same chatbot.
This process will be performed with chatbots created with the Taskyto framework
because this framework enables the reporting of the activated modules.
With these reports, it will be possible to measure
the coverage obtained during the \ac{TRACER} and SENSEI executions independently.

Higher coverage is anticipated during SENSEI profile execution,
as profiles are designed to systematically test all discovered parameters
and their valid combinations.
For instance, \ac{TRACER} may identify that a pizzeria chatbot
offers six pizza types, but does not try them all during exploration.
Instead, the generated profiles are designed to test all options.

Evaluating RQ1 is crucial for understanding
how thoroughly \ac{TRACER} exercises the chatbotâ€™s functionality.
High coverage does not guarantee high quality,
but it implies that a significant part of a system has been tested,
increasing the confidence in its reliability
\autocite{ammannIntroductionSoftwareTesting2017}.


RQ2 assesses the practical effectiveness
of the proposed approach in detecting chatbot errors.
To this aim, mutation testing is employed
\autocite{demilloHintsTestData1978}
by introducing defects into correct chatbots to create mutants,
and testing whether the generated profiles detect these defects.
A profile successfully detects a mutation (i.e., a fault)
when the simulated user fails to complete a goal
(e.g., ordering a pizza type that was removed in the mutant)
or when an expected output is missing or incorrect.
Therefore, assessing RQ2 is important
for providing evidence that our approach produces
profiles that are sufficiently comprehensive
to reveal real-world chatbot errors.

RQ3 assesses \ac{TRACER}'s ability at modelling and synthesising
real-world deployed chatbots.
Unlike RQ1 and RQ2, which involves chatbots developed with the Taskyto framework
specifically for this evaluation,
RQ3 evaluates chatbots deployed for real-world applications
such as, universities, town halls or transportation companies.
To evaluate this, the model inferred by \ac{TRACER}
will be manually validated to measure the precision
of the discovered functionalities.

The experiment data is available at:
\url{https://github.com/Chatbot-TRACER/TRACER-evaluation}.

\input{chapters/061_rq1.tex}

\input{chapters/062_rq2.tex}
