% !TeX root = ../main.tex
% Add the above to each chapter to make compiling the PDF easier in some editors.

\chapter{Tool Support}\label{chapter:tool_support}

To ensure that the previous methodology can be reproduced
we have implemented \acf{TRACER} in an open-source Python package
to allow users to execute it from a \ac{CLI}.
In addition, we have developed a web application
that allows users to execute both \ac{TRACER} and SENSEI
without requiring knowledge of how to operate the command line.


\section{Implementation and Architecture}

\subsection{Core Framework: LangGraph}

As explained during the \autoref{sec:profile-generation} (\nameref{sec:profile-generation})
\ac{TRACER} relies on \acp{LLM},
this is why we used LangGraph \autocite{LangGraph} as our framework for development.
We chose LangGraph because
it allows to manage and orchestrate
complex agentic workflows with states,
it is also an industry standard with extensive documentation.
LangGraph allows us to orchestrate the different stages
and to keep complex states where we store
the inferred model and the fields generated for the profiles.
Currently, \ac{TRACER} allows OpenAI and Gemini models,
but thanks to LangGraph it would be straightforward to add other \ac{LLM} providers.

\subsection{Modular Architecture}

\ac{TRACER} can infer a chatbot model as long as
the chatbot is accessible through an interface, typically a \acs{REST} \acs{API}.
Currently, it provides access to communicate with chatbots made with different technologies,
such as Taskyto \autocite{sanchezcuadradoAutomatingDevelopmentTaskoriented2024}, Rasa \autocite{Rasa2020} or 1MillionBot \autocite{1MillionBot}.
In addition, new connectors could be added by extending the current implementation.

Apart from these connectors,
\ac{TRACER} is divided into three modules,
each corresponding to a phase of the methodology:

\begin{itemize}
  \item \textbf{Explorer Module:}
    Contains the Explorer Agent
    and implements the logic for the Exploration Phase (see \autoref{sec:exploration}),
    managing the conversational sessions and the initial extraction of Functionality Nodes.

  \item \textbf{Refinement Module:}
    Implements the logic for the Refinement Phase (see \autoref{sec:refinement}),
    responsible for consolidating functionalities,
    classifying the chatbot, and inferring the final workflow structure.

  \item \textbf{Profile Generation Module:}
    Implements the seven-step synthesis process (see \autoref{sec:profile-generation}),
    taking the final chatbot model and generating the YAML user profiles.
\end{itemize}

\subsection{Generated Artefacts}

Upon completion, \ac{TRACER} generates the following artefacts
containing the results from the full analysis performed on the target chatbot:

\begin{itemize}
  \item A set of user profiles
    representing realistic users that would use the application
    and that will act as test cases
    (see \autoref{code:yaml-profile-pizza} and \autoref{code:yaml-profile-drinks})
  \item A markdown report containing the inferred model information such as
    the discovered functionalities, fallback message, language
    and other information such as token usage,
    number of \ac{LLM} calls or estimated cost.
  \item A graph representing the inferred model's workflow (see \autoref{fig:pizzeria-workflow})
  \item A JSON file containing the same workflow but in a textual format.
\end{itemize}



\section{Distribution and Development Workflow}

Before detailing the \ac{CLI}'s functioning,
this section will describe \ac{TRACER}'s packaging, distribution,
and the software engineering practices used to maintain its quality.
\ac{TRACER} is packaged and distributed as a package on the \acf{PyPI} repository
(\url{https://pypi.org/project/chatbot-tracer/}),
facilitating its installation by running
\texttt{pip install chatbot-tracer}.
This approach not only simplifies its use,
but also its implementation into other projects
such as the web application that was developed,
or other projects that could use \ac{TRACER}
since it can be added as another package requirement.

To ensure code quality and automate the release process
\ac{TRACER} makes use of GitHub Actions for the \ac{CI/CD} pipeline.
For the \ac{CI} we used Ruff \autocite{Ruff}.
Ruff is Python linter and formatter written in Rust
that combines tools like Flake8 or Black into a single and faster tool.
We used of Ruff not only to enforce a consistent code style,
but also to enforce code quality standards,
such as ensuring proper documentation
and managing code complexity by setting thresholds for metrics like McCabe's cyclomatic complexity.
For the \ac{CD} side,
we implemented a pipeline that whenever a tag with the format \texttt{v*.*.*} is published
automatically builds the package,
publishes it to \ac{PyPI},
and creates the corresponding GitHub release.
All the \ac{TRACER} source code can be accessed in \url{https://github.com/Chatbot-TRACER/TRACER}.

\section{Chatbot Connectors}

A key component of \ac{TRACER} is its connector system,
developed as part of this work and
packaged as the \texttt{chatbot-connectors} library.
The \texttt{chatbot-connectors} library is available on PyPI
(\url{https://pypi.org/project/chatbot-connectors/})
and its source code can be accessed at
\url{https://github.com/Chatbot-TRACER/chatbot-connectors}.

The purpose of this package is to provide a unified interface
to interact with different chatbots,
having the same method for sending messages
but different implementations for each chatbot technology.
For this we have an abstract base class
from which every implementation inherits.
This base class standardizes the core methods
for sending messages and receiving responses.

\subsection{Available Connector Technologies}

\ac{TRACER} currently supports four connector technologies,
each designed to interface with specific chatbot platforms:
Custom (a flexible YAML-configured connector for any chatbot \ac{API}),
MillionBot (specialized for the MillionBot platform),
Rasa (interfacing with Rasa chatbots through \ac{REST} webhooks),
and Taskyto (dedicated connector for Taskyto chatbot servers).

The Custom connector deserves special attention due to its flexibility,
providing a solution for integrating with any chatbot \ac{API}
without requiring custom code development.
This connector uses YAML configuration files
to define how to communicate with a chatbot's \ac{API},
specifying the \ac{API} endpoint, request structure,
authentication requirements, and response parsing instructions.

\subsection{Connector Discovery and Configuration}

The \ac{CLI} provides built-in commands to discover available connectors
and their configuration requirements:

\begin{lstlisting}[
language=tracer-examples,
caption={Listing available connectors.},
label={code:list-connectors}
]
$ tracer --list-connectors
Available Chatbot Connector Technologies:
==================================================
  
  - custom
    Description: Custom chatbot connector configured by a YAML file
    Use: --technology custom
    Parameters: --list-connector-params custom

  - millionbot
    Description: MillionBot chatbot connector
    Use: --technology millionbot
    Parameters: --list-connector-params millionbot

  - rasa
    Description: RASA chatbot connector using REST webhook
    Use: --technology rasa
    Parameters: --list-connector-params rasa

  - taskyto
    Description: Taskyto chatbot connector
    Use: --technology taskyto
    Parameters: --list-connector-params taskyto

Total: 4 connector(s) available
\end{lstlisting}

For detailed parameter information for a specific connector,
users can query the requirements:

\begin{lstlisting}[
language=tracer-examples,
caption={Listing connector parameters for Taskyto.},
label={code:list-connector-params}
]
$ tracer --list-connector-params taskyto
Parameters for 'taskyto' chatbot:
--------------------------------------------------
  - Name: base_url
    Type: string
    Required: True
    Description: The base URL of the Taskyto server.

  - Name: port
    Type: integer
    Required: False
    Default: 5000
    Description: The port of the Taskyto server.

Example usage:
  JSON format: --connector-params '{"base_url": "http://localhost"}'
  Key=Value format: --connector-params "base_url=http://localhost"
\end{lstlisting}

\subsection{Custom YAML Connector Configuration}

The Custom connector deserves special attention due to its flexibility.
It allows users to integrate with any chatbot \ac{API}
by creating a YAML configuration file that describes
the \ac{API} communication protocol.

A Custom connector configuration file contains the following key fields:

\begin{itemize}
    \item \texttt{name}: A friendly name for the chatbot (optional)
    \item \texttt{base\_url}: The base \acs{URL} of the chatbot \ac{API} (required)
    \item \texttt{send\_message.path}: \ac{API} endpoint path appended to base\_url (required)
    \item \texttt{send\_message.method}: \acs{HTTP} method (POST, GET, PUT, DELETE; default: POST)
    \item \texttt{send\_message.headers}: Custom headers including authentication (optional)
    \item \texttt{send\_message.payload\_template}: \acs{JSON} structure with \texttt{\{user\_msg\}} placeholder (required)
    \item \texttt{response\_path}: Dot-separated path to extract the bot's reply from the \acs{JSON} response (required)
\end{itemize}

\subsubsection{Configuration Examples}

A simple echo bot configuration demonstrates the basic structure:

\begin{lstlisting}[
language=connector-examples,
caption={Simple Custom connector configuration.},
label={code:simple-yaml-config}
]
name: "Echo Bot"
base_url: "https://postman-echo.com"
send_message:
  path: "/post"
  method: "POST"
  payload_template:
    message: "{user_msg}"
response_path: "json.message"
\end{lstlisting}

A more complex configuration with authentication:

\begin{lstlisting}[
language=connector-examples,
caption={Custom connector configuration with authentication.},
label={code:auth-yaml-config}
]
name: "Secure Bot"
base_url: "https://api.mychatbot.com"
send_message:
  path: "/chat/send"
  method: "POST"
  headers:
    Authorization: "Bearer your-api-key"
    Content-Type: "application/json"
  payload_template:
    query: "{user_msg}"
    session_id: "user123"
response_path: "response.text"
\end{lstlisting}

The \texttt{response\_path} field uses dot notation to navigate through \ac{JSON} responses:
\begin{itemize}
  \item \texttt{"message"} accesses \texttt{response["message"]}
  \item \texttt{"data.text"} accesses \texttt{response["data"]["text"]}
  \item \texttt{"results.0.content"} accesses
\texttt{response["results"][0]["content"]}
\end{itemize}


\section{The \acl{CLI}}

The primary method for using \ac{TRACER} is through the \ac{CLI}.
The entire \ac{TRACER} pipeline can be executed,
from chatbot exploration to profile generation,
through a single command.
This approach enables users who prefer terminal-based workflows
such as developers, to execute TRACER easily.
It also enables the integration of \ac{TRACER} within other projects.

\ac{TRACER} is run by one main command: \texttt{tracer}.
To see in more detail its options, users can run \texttt{tracer --help}.
Some of the key arguments are as follows:

\subsection{Conversation Control}

\texttt{--sessions} or \texttt{-s} that controls the number of conversations
that \ac{TRACER} will have with the chatbot under testing
and \texttt{--turns} or \texttt{-n} for the number of turns or steps per conversation.

\subsection{Connector Configuration}

The arguments \texttt{--technology} or \texttt{-t} allow users to select
from the available connector technologies (custom, millionbot, rasa, taskyto).
Additional connector-specific parameters can be provided using
\texttt{--connector-params} or \texttt{-cp} in either \ac{JSON} format or key=value pairs
separated by commas.
For the Custom connector, users specify the YAML configuration file path.

\subsection{LLM Configuration}

Using \texttt{--model} or \texttt{-m} allows the user to select the \acl{LLM}
used for the exploration and analysis,
then \texttt{--profile-model} or \texttt{-pm} is an optional argument
that if set will make the generated user profiles' \ac{LLM} be the one specified,
otherwise the \ac{LLM} that will appear in the profiles
will be the same one used for \ac{TRACER},
it is recommended to use a more capable model for exploration and analysis
since will infer a more comprehensive model with more realistic profiles,
and then a more economical model to run the profiles
since there will be many more \ac{LLM} calls
and the chosen model will not have as significant an impact.

\subsection{Output and Logging}

Three verbose levels are also available:
the basic one where only key information, such as the discovered functionalities,
the current session or phase, and any warnings.
The verbose level, activated with \texttt{-v},
which displays the conversations in addition to the previous information,
and the debug \texttt{-vv}
which displays the same information as the verbose level,
in addition to details such as prompts
sent to the \ac{LLM}, its responses,
and other logs that may have been generated
during the development and debugging of the program.
Lastly, the \texttt{--output} or \texttt{-o},
which controls where all the generated artefacts will be placed.

\begin{lstlisting}[
language=tracer-examples,
caption={TRACER command example with Taskyto connector.},
label={code:tracer-command-example}
]
$ tracer --technology taskyto --connector-params "base_url=http://localhost" --sessions 12 --turns 8 --model gemini-2.5-flash --profile-model gemini-2.0-flash --output ./pizzeria_results -v
\end{lstlisting}

The command in \autoref{code:tracer-command-example}
demonstrates a typical execution of \ac{TRACER} against a Taskyto-based pizzeria chatbot.
It is configured to run 12 exploration sessions of 8 turns each.
For the exploration, analysis and profile generation,
the more advanced model Gemini 2.5 Flash will be used,
then for the model that is defined in each user profile, Gemini 2.0 Flash will be used since it is faster and cheaper.
We used the \texttt{-v} to monitor the conversations occurring
between the explorer agent and the chatbot under testing.
Finally, all the artefacts will be stored in a directory called \texttt{pizzeria\_results}.

\begin{lstlisting}[
language=tracer-examples,
caption={TRACER command example with Custom YAML connector.},
label={code:tracer-yaml-example}
]
$ tracer --technology custom --connector-params "config_path=./my-bot-config.yml" --sessions 10 --turns 6 --model gpt-4o --output ./yaml_bot_results
\end{lstlisting}

\autoref{code:tracer-yaml-example} shows how to use \ac{TRACER}
with a custom chatbot through the YAML connector,
demonstrating the flexibility of the connector system
in accommodating various chatbot technologies.

\section{The Web Application}

To complement the \ac{CLI},
we developed a web application
to provide a user-friendly interface for running the whole end-to-end pipeline,
that is, to run \ac{TRACER} to generate the user profiles,
and then to execute these with SENSEI.
This, enables a broader audience to use both \ac{TRACER} and SENSEI
without the need of knowing how to use the \ac{CLI}.

\subsection{System Architecture}

The web application is built on a modern,
multi-tiered architecture designed for scalability, security, and asynchronous processing.
\autoref{fig:app-architecture} provides an overview of this architecture,
illustrating the relationships between the five key layers:
the Client, Presentation, Application, Task Processing, and Data layers.
The following subsections will provide a detailed explanation
of the technologies and design patterns used within each of these layers.

\begin{figure}[!htbp]
    \centering
    \input{figures/web_architecture.tex}
    \caption{Web Application Architecture and Connections.}
    \label{fig:app-architecture}
\end{figure}

\subsection{Core Technology Stack}

The backend of the application was developed in Django \autocite{Django},
This framework was chosen because it is Python-based
and therefore compatible with \ac{TRACER} and SENSEI,
and also it offers the Django REST Framework \autocite{DjangoRESTFramework}
which enables the development of a \ac{REST} \ac{API}
that will be consumed by our frontend.

For the frontend we chose React \autocite{React},
a JavaScript library to develop \acp{SPA}
which consumes data directly from our Django \ac{API}.

Lastly, to ensure data persistence,
we used PostgreSQL \autocite{PostgreSQL2025},
we chose a \ac{SQL} database since Django's \ac{ORM} supports this type of databases out of the box,
also, we preferred PostgreSQL over the default Django's SQLite
since the latter is more oriented to development and testing
and stores everything in a single file which causes concurrency and performance issues when used in production.

\subsection{Asynchronous Task Handling}

\ac{TRACER} and SENSEI executions both take from a few minutes up to hours,
thus, executing this tasks synchronously would leave the user's interface unresponsive
for the duration of the whole process.
To handle this we implemented asynchronous executions,
we did this by using a distributed task queue called Celery \autocite{Celery},
which enables these jobs to be handled asynchronously.
Then, to communicate Django with Celery we need a broker,
for this we used RabbitMQ \autocite{RabbitMQ}.
These two tools in conjunction will allow the user to execute \ac{TRACER} or SENSEI
and change to a different view, log out, or even turn off the computer
and later return to check its progress.
It will also prevent server overload
since we can limit the number of concurrent jobs
and if there are requests to execute more than this number,
the jobs will be waiting on the queue instead of saturating the server.


\subsection{The Nginx Reverse Proxy}

\begin{figure}[htbp]
    \centering
    \input{figures/nginx_routing.tex}
    \caption{Nginx routing to serve React, Django's static files and Django API.}
    \label{fig:nginx-routing}
\end{figure}

A reverse proxy is essential for managing the communication between
the user, the React frontend, and the Django backend.
For this role, we used Nginx \autocite{Nginx}.
Its primary responsibility is to route incoming HTTP requests to the appropriate service based on the URL path.
This also addresses the challenge of accessing each service in a different port,
although they are running in a different port
we can always access the same URL and Nginx will redirect the call.

In the \autoref{fig:nginx-routing}
we can see the configuration of the reverse proxy.
It recieves all the calls from the users and routes them to the correct service.
We have that all calls made to \texttt{/api/}, \texttt{/admin/} and \texttt{/filevault/}
get redirected to Django's backend since as it processes \ac{API} calls,
return the different files and show the admin dashboard.
The Django admin dashboard requires its CSS to be rendered correctly,
which in turn necessitates serving the Django static files.
For this reason, any request made to the \texttt{/static/} path
is redirected to the location of these files.
Lastly, the remaining calls will be redirected to React's build files.

\subsection{Deployment and Containerisation}



To facilitate the deployment of these services
we containerized the whole web application using Docker \autocite{merkelDockerLightweightLinux2014} along Docker compose.
This was done not only to simplify the deployment process,
but to ensure that no matter the machine and the dependencies it had installed,
it would work if it had Docker.
We made two different Docker compose versions, one for development and one for production,
each with their own \ac{DB}, message broker, and so on.
So that production data was not affected during development.
The complete production architecture is shown in \autoref{fig:docker-architecture}.

\begin{figure}[!htbp]
    \centering
    \input{figures/docker_setup.tex}
    \caption{Docker Container Architecture.}
    \label{fig:docker-architecture}
\end{figure}

In the \autoref{fig:docker-architecture} we show how the production containers and volumes are organized.
Each square is a container running said service,
then each cylinder is volume, where files are stored to ensure data persistance when containers are taken down.
Each blue arrow means that the volume is mounted in that container, i.e., the container can access the volume's files,
and the red ones show dependencies for the Docker compose,
which means that until the container that the arrow is pointing at is not working,
the container at the start of the arrow will not be initiated.

This ensures that until we do not have the \ac{DB} and message broker set
the queue and backend will not start,
and only then the frontend will be built
and lastly the Nginx reverse proxy.
In the development version the setup was mostly the same
but we had another volume that had the frontend and backend code
so that we would have hot reload (i.e., changes in the code were shown live in the web).

\subsection{Security Measures}

To ensure the application's security we followed the Django Deployment Checklist \autocite{DeploymentChecklistDjango}.
Related to the authentication we implemented a new user model,
and to handle the authentication we used Django-Rest-Knox \autocite{DjangoRestKnox} tokens.
This is a library that implements an improved token system over the default in the Django REST Framework \autocite{DjangoRESTFramework},
the main improvements include:
\begin{itemize}
  \item \textbf{One token per device:}
    instead of having one token per user
    which would mean that logging in from different devices
    would mean that they share the same token,
    causing that for example
    logging out on one device would terminate the session on all others.
    Knox solves this by creating one token per device.

  \item \textbf{Encrypted token in \ac{DB}:}
    this is a major security flaw from \ac{DRF}
    and it is that tokens are stored as plain text in the \ac{DB},
    meaning that if the \ac{DB} was compromised,
    the attacker could use those tokens to impersonate the legitimate user.

  \item \textbf{Token expire time:}
    in \ac{DRF}, once an account is authenticated, the generated token remains valid indefinitely.
    Knox, in contrast, allows an expiry time to be set.
    After this period, the session is terminated as the token becomes invalid.
\end{itemize}

To store the \ac{API} keys uploaded by the user to run \ac{TRACER} or SENSEI
we used Fernet Symmetric Encryption \autocite{FernetSymmetricEncryption}
to encrypt them before saving them to the \ac{DB}.
Although the \ac{DB} has a user and password (not the default ones)
we determined this was a prudent measure to add further layers of security,
the same way that Knox encrypts, so that
if the database was compromised, these credentials would remain encrypted.

These measures, combined with strict access controls,
ensure the application adheres to robust security practices.
Every request to the \ac{API} requires token validation.
Furthermore, models are configured to be readable only by their owner,
unless explicitly set to public.
Even when a project is public, write access remains restricted to the owner.
