% !TeX root = ../main.tex
% Add the above to each chapter to make compiling the PDF easier in some editors.

\chapter{Tool Support}\label{chapter:tool_support}

To ensure that the previous methodology can be reproduced
we have implemented \acf{TRACER} in an open-source Python package
to allow users to execute it from a \ac{CLI}.
On top of that, we have developed a web application
that allows users to execute both \ac{TRACER} and SENSEI
without the need of knowing how to operate the command line.


\section{Implementation and Architecture}

\subsection{Core Framework: LangGraph}

As explained during the \autoref{sec:profile-generation} (\nameref{sec:profile-generation})
\ac{TRACER} relies on \acp{LLM},
this is why used LangGraph \autocite{LangGraph} as our framework for development.
The reason why we chose LangGraph
was because it allows to manage and orchestrate
complex agentic workflows with states,
it is also an industry standard and
it has an extensive documentation.
LangGraph allows us to orchestrate the different stages
and to keep complex states where we store
the inferred model and the fields generated for the profiles.
Right now \ac{TRACER} allows OpenAI and Gemini models,
but thanks to LangGraph it would be easy to add other \ac{LLM} providers.

\subsection{Modular Architecture}

\ac{TRACER} can infer a chatbot model as long as
the chabots is accessible through an interface, typically a \acs{REST} \acs{API}.
Right now it provides access to communicate with chatbots made with different technologies,
such as Taskyto \autocite{sanchezcuadradoAutomatingDevelopmentTaskoriented2024}, Rasa \autocite{Rasa2020} or 1MillionBot \autocite{1MillionBot}.
In addition, new connectors could be added by extending the current implementation.

Apart from these connectors,
\ac{TRACER} is divided into three modules,
each corresponding to a phase of the methodology:

\begin{itemize}
  \item \textbf{Explorer Module:}
    Contains the Explorer Agent
    and implements the logic for the Exploration Phase (see \autoref{sec:exploration}),
    managing the conversational sessions and the initial extraction of Functionality Nodes.

  \item \textbf{Refinement Module:}
    Implements the logic for the Refinement Phase (see \autoref{sec:refinement}),
    responsible for consolidating functionalities,
    classifying the chatbot, and inferring the final workflow structure.

  \item \textbf{Profile Generation Module:}
    Implements the seven-step synthesis process (see \autoref{sec:profile-generation}),
    taking the final chatbot model and generating the YAML user profiles.
\end{itemize}

\subsection{Generated Artifacts}

Upon completion, \ac{TRACER} generates the following artifacts
containing the results from the full analysis performed on the target chatbot:

\begin{itemize}
  \item A set of user profiles
    representing realistic users that would use the application
    and that will act as test cases
    (see \autoref{code:yaml-profile-pizza} and \autoref{code:yaml-profile-drinks})
  \item A markdown report containing the inferred model information like
    the discovered functionalities, fallback message, language
    and also other information like token usage, number of \ac{LLM} calls or estimated cost.
  \item A graph representing the inferred model's workflow (see \autoref{fig:pizzeria-workflow})
  \item A JSON file containing the same workflow but in text.
\end{itemize}



\section{Distribution and Development Workflow}

Before detailing the \ac{CLI}'s functioning,
this section will describe \ac{TRACER}'s packaging, distribution,
and the software engineering practices used to maintain its quality.
\ac{TRACER} is packaged and distributed as a package on the \acf{PyPI} repository
(\url{https://pypi.org/project/chatbot-tracer/}),
making it easy to install by running
\texttt{pip install chatbot-tracer}.
This not only makes it easy to use,
but also makes it easy to implement into other projects
such as the web application done,
or other projects that could use \ac{TRACER}
since it can just be added as another package requirement.

To ensure code quality and automate the release process
\ac{TRACER} makes use of GitHub Actions for the \ac{CI/CD} pipeline.
For the \ac{CI} we made use of Ruff \autocite{Ruff}.
Ruff is Python linter and formatter written in Rust
that combines tools like Flake8 or Black into a single and faster tool.
We made use of Ruff not only to enforce a consistent code style,
but also to enforce code quality standards,
such as ensuring proper documentation
and managing code complexity by setting thresholds for metrics like McCabe's cyclomatic complexity.
For the \ac{CD} side,
we implemented a pipeline that whenever a tag with the format \texttt{v*.*.*} is published
automatically builds the package,
publishes it to \ac{PyPI},
and creates the corresponding GitHub release.
All the \ac{TRACER} source code can be accessed in \url{https://github.com/Chatbot-TRACER/TRACER}.

\section{The \acl{CLI}}

The primary way of using \ac{TRACER} is through the \ac{CLI}.
We can execute the whole \ac{TRACER} pipeline,
from chatbot exploration to profile generation,
through a single command.
This way enables users who prefer terminal-based workflows
such as developers, to execute TRACER easily.
It also allows \ac{TRACER} to be integrated within other projects.

\ac{TRACER} is run by one main command: \texttt{tracer}.
To see in more detail its options, users can run \texttt{tracer --help}.
Some of the key arguments are the following ones:

\subsection{Conversation Control}

\texttt{--sessions} or \texttt{-s} that controls the number of conversations
that \ac{TRACER} will have with the chatbot under testing
and \texttt{--turns} or \texttt{-n} for the number of turns or steps per conversation.

\subsection{Connector Configuration}

The arguments \texttt{--technology} or \texttt{-t} along with \texttt{--url} or \texttt{-u}
allow to configure the connector by choosing
the chatbot technology along with the \ac{API} endpoint.

\subsection{LLM Configuration}

Using \texttt{--model} or \texttt{-m} lets one decide the \acl{LLM} used for the exploration and analysis,
then \texttt{--profile-model} or \texttt{-pm} is an optional argument
that if set will make the generated user profiles' \ac{LLM} be the one specified,
otherwise the \ac{LLM} that will appaer in the profiles will be the same one used for \ac{TRACER},
it is recommended to use a better model for exploration and analysis
since will infer a more comprehensive model with more realistic profiles,
and then a more economic model to run the profiles
since there will be many more \ac{LLM} calls
and the chosen model will not have that much impact.

\subsection{Output and Logging}

We also have the verbose levels which can be three levels:
the basic one where we will just see things like the discovered functionalities,
which session or phase are we in and warnings.
The verbose level, activated with \texttt{-v},
which allows us to see the conversation too,
and the debug \texttt{-vv} that will show what the verbose shows plus information like
prompts sent to the \ac{LLM}, its responses,
and other logs that may have been set during the development and debugging of the program.
Lastly, we have \texttt{--output} or \texttt{-o},
which simply controls where all the generated artifacts will be placed.

\begin{lstlisting}[
language=tracer-examples,
caption={Tracer command example.},
label={code:tracer-command-example}
]
$ tracer -t taskyto -u http://localhost:5000 -s 12 -n 8 -m gemini-2.5-flash -pm gemini-2.0-flash -o ./pizzeria_results -v
\end{lstlisting}

The command in \autoref{code:tracer-command-example}
demonstrates a typical execution of \ac{TRACER} against a Taskyto-based pizzeria chatbot.
It is configured to run 12 exploration sessions of 8 turns each.
For the exploration, analysis and profile generation, the smarter model Gemini 2.5 Flash will be used,
then for the model that is defined in each user profile, Gemini 2.0 Flash will be used since it is faster and cheaper.
We used the \texttt{-v} to be able to monitor the conversations that are happening
between the explorer agent and the chatbot under testing.
Finally, all the artifacts will be stored in a directory called \texttt{pizzeria\_results}.

\section{The Web Application}

To complement the \ac{CLI},
we developed a web application
to provide a user friendly interface that will allow to run the whole end-to-end pipeline,
that it is, to run \ac{TRACER} to generate the user profiles,
and then to execute these with SENSEI.
This, allows a broader audience to use both \ac{TRACER} and SENSEI
without the need of knowing how to use the \ac{CLI}.

\subsection{System Architecture}

The web application is built on a modern,
multi-tiered architecture designed for scalability, security, and asynchronous processing.
\autoref{fig:app-architecture} provides an overview of this architecture,
illustrating the relationships between the five key layers:
the Client, Presentation, Application, Task Processing, and Data layers.
The following subsections will provide a detailed explanation
of the technologies and design patterns used within each of these layers.

\begin{figure}[!htbp]
    \centering
    \input{figures/web_architecture.tex}
    \caption{Web Application Architecture and Connections.}
    \label{fig:app-architecture}
\end{figure}

\subsection{Core Technology Stack}

The backend of the application was developed in Django \autocite{Django},
the reason why this framework was used is because it is made for Python
so it suits both \ac{TRACER} and SENSEI,
and also it offers the Django REST Framework \autocite{DjangoRESTFramework}
that will allow us to develop a \ac{REST} \ac{API} that will be consumed by our frontend.

For the frontend we went with React \autocite{React},
a JavaScript library to develop \acp{SPA}
that will consume directly from our Django \ac{API}.

Lastly, to ensure data persistance,
we used PostgreSQL \autocite{PostgreSQL2025},
we chose a \ac{SQL} database since Django's \ac{ORM} supports this type of databases out of the box,
also, we preferred PostgreSQL over the default Django's SQLite
since the latter is more oriented to development and testing
and stores everything in a single file which causes concurrency and performance issues when used in production.

\subsection{Asynchronous Task Handling}

\ac{TRACER} and SENSEI executions both take from a few minutes up to hours,
thus, executing this tasks synchronously would leave the user's interface frozen
for the duration of the whole process.
To handle this we implemented asynchronous executions,
we did this by using a distributed task queue called Celery \autocite{Celery},
which lets us handle these jobs asynchronously.
Then, to communicate Django with Celery we need a broker,
for this we used RabbitMQ \autocite{RabbitMQ}.
These two tools in conjunction will allow the user to execute \ac{TRACER} or SENSEI
and change to a different view, log out, or even turn off the computer
and then come back to check the progress.
It will also help the server to not get saturated
since we can limit the number of concurrent jobs
and if there are requests to execute more than this number,
the jobs will be waiting on the queue instead of saturating the server.


\subsection{The Nginx Reverse Proxy}

\begin{figure}[htbp]
    \centering
    \input{figures/nginx_routing.tex}
    \caption{Nginx routing to serve React, Django's static files and Django API.}
    \label{fig:nginx-routing}
\end{figure}

A reverse proxy is essential for managing the communication between
the user, the React frontend, and the Django backend.
For this role, we used Nginx \autocite{Nginx}.
Its primary responsibility is to route incoming HTTP requests to the appropriate service based on the URL path.
This also solves the issue of accessing each service in a different port,
although they are running in a different port
we can just access always the same URL and Nginx will redirect the call.

In the \autoref{fig:nginx-routing}
we can see the configuration of the reverse proxy.
It recieves all the calls from the users and routes them to the correct service.
We have that all calls made to \texttt{/api/}, \texttt{/admin/} and \texttt{/filevault/}
get redirected to Django's backend since it is who will process \ac{API} calls,
return the different files and show the admin dashboard,
although to show the admin dashboard's CSS
we need to collect and serve the Django static files,
which is why calls made to \texttt{/static/} will get redirected to Django's static files.
Lastly, the remaining calls will be redirected to React's build files.

\subsection{Deployment and Containarization}



To be able to deploy all of these services we containerized the whole web application using Docker \autocite{merkelDockerLightweightLinux2014} along Docker compose.
This was done not only to simplify the deployment process,
but to ensure that no matter the machine and the dependencies it had installed,
it would work if it had Docker.
We made two different Docker compose versions, one for development and one for production,
each with their own \ac{DB}, message broker, etc. So that production data was not affected during development.
The complete production architecture is shown in \autoref{fig:docker-architecture}.

\begin{figure}[!htbp]
    \centering
    \input{figures/docker_setup.tex}
    \caption{Docker Container Architecture.}
    \label{fig:docker-architecture}
\end{figure}

In the \autoref{fig:docker-architecture} we show how the production containers and volumes are organized.
Each square is a container running said service,
then each cylinder is volume, where files are stored to ensure data persistance when containers are taken down.
Each blue arrow means that the volume is mounted in that container, i.e., the container can access the volume's files,
and the red ones show dependencies for the Docker compose,
which means that until the container that the arrow is pointing at is not working,
the one that is pointing (has the start of the arrow) is not going to get started.

This ensures that until we don't have the \ac{DB} and message broker set
the queue and backend won't start,
and only then the frontend will get build
and lastly the Nginx reverse proxy.
In the development version the setup was mostly the same
but we had another volume that had the frontend and backend code
so that we would have hot reload (i.e., changes in the code were shown live in the web).

\subsection{Security Measures}

To ensure the application's security we followed the Django Deployment Checklist \autocite{DeploymentChecklistDjango}.
Related to the authentication we implemented a new user model,
and to handle the authentication we used Django-Rest-Knox \autocite{DjangoRestKnox} tokens.
This is a library that implements a improved token system over the default in the Django REST Framework \autocite{DjangoRESTFramework},
the main improvements that it brings are:
\begin{itemize}
  \item \textbf{One token per device:}
    instead of having one token per user
    which would mean that logging in from different devices
    would mean that they share the same token,
    causing that for example closing session in one device would close it in all the others.
    Knox solves this by creating one token per device.

  \item \textbf{Encrypted token in \ac{DB}:}
    this is a major security flaw from \ac{DRF}
    and it is that tokens are stored as plain text in the \ac{DB},
    meaning that if the \ac{DB} was compromised,
    the attacker could use those tokens to act like he was logged in the account he wanted.

  \item \textbf{Token expire time:}
    in \ac{DRF} once you log in with an account that token is valid for ever.
    Meanwhile, in Knox you can set a expiracy time which will cause that after that time,
    your session will be closed since the token won't be valid.
\end{itemize}

To store the \ac{API} keys uploaded by the user to run \ac{TRACER} or SENSEI
we used Fernet Symmetric Encryption \autocite{FernetSymmetricEncryption}
to encrypt them before saving them to the \ac{DB}.
Although the \ac{DB} has a user and password (not the default ones)
we decided this was a good idea to have more layers of security,
the same way that Knox encrypts, so that if someone managed to breach the \ac{DB}
those things would still be encrypted.

These things combined with the fact that
every time that a petition is made to the \ac{API} the token is validated,
and that the models are configured so that they can only be
read by the owner unless it is set to be public (one can set projects as public)
but that even if it is public it can only be written by the owner,
ensure that the application follows good security practices.
