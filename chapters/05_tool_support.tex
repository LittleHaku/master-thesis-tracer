% !TeX root = ../main.tex
% Add the above to each chapter to make compiling the PDF easier in some editors.

\chapter{Tool Support}\label{chapter:tool_support}

To ensure that the previous methodology can be reproduced
we have implemented \acf{TRACER} in an open-source Python package
to allow users to execute it from a \ac{CLI}.
On top of that, we have developed a web application
that allows users to execute both \ac{TRACER} and Sensei
without the need of knowing how to operate the command line.


\section{Implementation and Architecture}

\subsection{Core Framework: LangGraph}

As explained during the \autoref{sec:profile-generation} (\nameref{sec:profile-generation})
\ac{TRACER} relies on \acp{LLM},
this is why used LangGraph \autocite{LangGraph} as our framework for development.
The reason why we chose LangGraph
was because it allows to manage and orchestrate
complex agentic workflows with states,
it is also an industry standard and
it has an extensive documentation.
LangGraph allows us to orchestrate the different stages
and to keep complex states where we store
the inferred model and the fields generated for the profiles.
Right now \ac{TRACER} allows OpenAI and Gemini models,
but thanks to LangGraph it would be easy to add other \ac{LLM} providers.

\subsection{Modular Architecture}

\ac{TRACER} can infer a chatbot model as long as
the chabots is accessible through an interface, typically a \acs{REST} \acs{API}.
Right now it provides access to communicate with chatbots made with different technologies,
such as Taskyto \autocite{sanchezcuadradoAutomatingDevelopmentTaskoriented2024}, Rasa \autocite{Rasa2020} or 1MillionBot \autocite{1MillionBot}.
In addition, new connectors could be added by extending the current implementation.

Apart from these connectors,
\ac{TRACER} is divided into three modules,
each corresponding to a phase of the methodology:

\begin{itemize}
  \item \textbf{Explorer Module:}
    Contains the Explorer Agent
    and implements the logic for the Exploration Phase (see \autoref{sec:exploration}),
    managing the conversational sessions and the initial extraction of Functionality Nodes.

  \item \textbf{Refinement Module:}
    Implements the logic for the Refinement Phase (see \autoref{sec:refinement}),
    responsible for consolidating functionalities,
    classifying the chatbot, and inferring the final workflow structure.

  \item \textbf{Profile Generation Module:}
    Implements the seven-step synthesis process (see \autoref{sec:profile-generation}),
    taking the final chatbot model and generating the YAML user profiles.
\end{itemize}

\subsection{Generated Artifacts}

Upon completion, \ac{TRACER} generates the following artifacts
containing the results from the full analysis performed on the target chatbot:

\begin{itemize}
  \item A set of user profiles
    representing realistic users that would use the application
    and that will act as test cases
    (see \autoref{code:yaml-profile-pizza} and \autoref{code:yaml-profile-drinks})
  \item A markdown report containing the inferred model information like
    the discovered functionalities, fallback message, language
    and also other information like token usage, number of \ac{LLM} calls or estimated cost.
  \item A graph representing the inferred model's workflow (see \autoref{fig:pizzeria-workflow})
  \item A JSON file containing the same workflow but in text.
\end{itemize}



\section{Distribution and Development Workflow}

Before detailing the \ac{CLI}'s functioning,
this section will describe \ac{TRACER}'s packaging, distribution,
and the software engineering practices used to maintain its quality.
\ac{TRACER} is packaged and distributed as a package on the \acf{PyPI} repository
(\url{https://pypi.org/project/chatbot-tracer/}),
making it easy to install by running
\texttt{pip install chatbot-tracer}.
This not only makes it easy to use,
but also makes it easy to implement into other projects
such as the web application done,
or other projects that could use \ac{TRACER}
since it can just be added as another package requirement.

To ensure code quality and automate the release process
\ac{TRACER} makes use of GitHub Actions for the \ac{CI/CD} pipeline.
For the \ac{CI} we made use of Ruff \autocite{Ruff}.
Ruff is Python linter and formatter written in Rust
that combines tools like Flake8 or Black into a single and faster tool.
We made use of Ruff not only to enforce a consistent code style,
but also to enforce code quality standards,
such as ensuring proper documentation
and managing code complexity by setting thresholds for metrics like McCabe's cyclomatic complexity.
For the \ac{CD} side,
we implemented a pipeline that whenever a tag with the format \texttt{v*.*.*} is published
automatically builds the package,
publishes it to \ac{PyPI},
and creates the corresponding GitHub release.
All the \ac{TRACER} source code can be accessed in \url{https://github.com/Chatbot-TRACER/TRACER}.

\section{The \acl{CLI}}

The primary way of using \ac{TRACER} is through the \ac{CLI}.
We can execute the whole \ac{TRACER} pipeline,
from chatbot exploration to profile generation,
through a single command.
This way enables users who prefer terminal-based workflows
such as developers, to execute TRACER easily.
It also allows \ac{TRACER} to be integrated within other projects.

\ac{TRACER} is run by one main command: \texttt{tracer}.
To see in more detail its options, users can run \texttt{tracer --help}.
Some of the key arguments are the following ones:

\subsection{Conversation Control}

\texttt{--sessions} or \texttt{-s} that controls the number of conversations
that \ac{TRACER} will have with the chatbot under testing
and \texttt{--turns} or \texttt{-n} for the number of turns or steps per conversation.

\subsection{Connector Configuration}

The arguments \texttt{--technology} or \texttt{-t} along with \texttt{--url} or \texttt{-u}
allow to configure the connector by choosing
the chatbot technology along with the \ac{API} endpoint.

\subsection{LLM Configuration}

Using \texttt{--model} or \texttt{-m} lets one decide the \acl{LLM} used for the exploration and analysis,
then \texttt{--profile-model} or \texttt{-pm} is an optional argument
that if set will make the generated user profiles' \ac{LLM} be the one specified,
otherwise the \ac{LLM} that will appaer in the profiles will be the same one used for \ac{TRACER},
it is recommended to use a better model for exploration and analysis
since will infer a more comprehensive model with more realistic profiles,
and then a more economic model to run the profiles
since there will be many more \ac{LLM} calls
and the chosen model will not have that much impact.

\subsection{Output and Logging}

We also have the verbose levels which can be three levels:
the basic one where we will just see things like the discovered functionalities,
which session or phase are we in and warnings.
The verbose level, activated with \texttt{-v},
which allows us to see the conversation too,
and the debug \texttt{-vv} that will show what the verbose shows plus information like
prompts sent to the \ac{LLM}, its responses,
and other logs that may have been set during the development and debugging of the program.
Lastly, we have \texttt{--output} or \texttt{-o},
which simply controls where all the generated artifacts will be placed.

\begin{lstlisting}[
language=bash,
caption={Tracer command example.},
label={code:tracer-command-example}
]
$ tracer -t taskyto -u http://localhost:5000 -s 12 -n 8 -m gemini-2.5-flash -pm gemini-2.0-flash -o ./pizzeria_results -v
\end{lstlisting}

The command in \autoref{code:tracer-command-example}
demonstrates a typical execution of \ac{TRACER} against a Taskyto-based pizzeria chatbot. 
It is configured to run 12 exploration sessions of 8 turns each.
For the exploration, analysis and profile generation, the smarter model Gemini 2.5 Flash will be used,
then for the model that is defined in each user profile, Gemini 2.0 Flash will be used since it is faster and cheaper.
We used the \texttt{-v} to be able to monitor the conversations that are happening
between the explorer agent and the chatbot under testing.
Finally, all the artifacts will be stored in a directory called \texttt{pizzeria\_results}.

\section{The Web Application}

To complement the \ac{CLI},
we developed a web application
to provide a user friendly interface that will allow to run the whole end-to-end pipeline,
that it is, to run \ac{TRACER} to generate the user profiles,
and then to execute these with Sensei.
This, allows a broader audience to use both \ac{TRACER} and Sensei
without the need of knowing how to use the \ac{CLI}.

\subsection{System Architecture}

\subsection{Core Technology Stack}

The backend of the application was developed in Django \autocite{Django},
the reason why this framework was used is because it is made for Python
so it suits both \ac{TRACER} and Sensei,
and also it offers the Django REST Framework \autocite{DjangoRESTFramework}
that will allow us to develop a \ac{REST} \ac{API} that will be consumed by our frontend.

For the frontend we went with React \autocite{React},
a JavaScript library to develop \acp{SPA}
that will consume directly from our Django \ac{API}.

Lastly, to ensure data persistance,
we used PostgreSQL \autocite{PostgreSQL2025},
we chose a \ac{SQL} database since Django's \ac{ORM} supports this type of databases out of the box,
also, we preferred PostgreSQL over the default Django's SQLite
since the latter is more oriented to development and testing
and stores everything in a single file which causes concurrency and performance issues when used in production.

\subsection{Asynchronous Task Handling}

\ac{TRACER} and Sensei executions both take from a few minutes up to hours,
thus, executing this tasks synchronously would leave the user's interface frozen
for the duration of the whole process.
To handle this we implemented asynchronous executions,
we did this by using a distributed task queue called Celery \autocite{Celery},
which lets us handle these jobs asynchronously.
Then, to communicate Django with Celery we need a broker,
for this we used RabbitMQ \autocite{RabbitMQ}.
These two tools in conjunction will allow the user to execute \ac{TRACER} or Sensei
and change to a different view, log out, or even turn off the computer
and then come back to check the progress.
It will also help the server to not get saturated
since we can limit the number of concurrent jobs
and if there are requests to execute more than this number,
the jobs will be waiting on the queue instead of saturating the server.


\subsection{The Nginx Reverse Proxy}
